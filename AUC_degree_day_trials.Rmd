---
title: "AUC_degree_day_trials"
author: "Sara DeLaurentis"
date: "5/11/2022"
output: html_document
---

```{r setup, include=FALSE}

library(stringr)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(zoo)


```

## 

```{r data setup}

## TEMPORARILY SET WORKING DIRECTORY TO "Itasca_project_19-21" !!!! ##

data <- read.csv("ALL.csv")

head(data)

data$date.time <- data$date.time %>% as.POSIXlt(tz = "") #set date/time class to POSIXlt for greater ease in parsing date elements.
head(data$date.time) 

# -- subset by date, add week-counting columns, calculate weekly means --

START <- as.POSIXlt("2020-10-01 00:00:00", tz = "") #trying the october 1-yr set to see if the code works. 5-5, SD
#BREAK <- as.POSIXlt("2020-08-25", tz = "")
#RESUME <- as.POSIXlt("2020-09-30", tz = "")
END <- as.POSIXlt("2021-10-01 00:00:00", tz = "")

data[is.na(data$value),] %>% #there should be a handful of NA's present if you're working with the right version of .csv files.
  print()

data <- data %>% 
  subset.data.frame(date.time >= START & date.time <= END) #subset the whole df                         
    # Apply filter & !is.na

# remove partial datasets for the given window. Anything with less than 500 values gets removed.
data <- data %>%
  group_by(site, rep, position) %>% 
  mutate(length = length(date.time)) %>% 
  filter(length > 500)


```


```{r AUC trial one}

#  https://stackoverflow.com/questions/4954507/calculate-the-area-under-a-curve

x <- 1:10
y <- 3*x+25 #y = 3 times x, +25.
id <- order(x) #an ordered x
print(id)
str(id)
AUC <- sum(diff(x[id])*rollmean(y[id],2)) 

AUC.mat <- c(diff(x[id])*rollmean(y[id],2))
view(AUC.mat)

#translation: the sum of: the lagged (by 1) difference of x[id] times the mean of the two y values. Here, the difference is only 1. Each polygon is 1 wide by the y-mean high.

#how to adapt this to date.time measurements taken every 4 hours?
#each polygon is the time difference by the mean between the two temperature points high.

print(AUC)
?rollmean


#Best first guess:

AUC <- sum(diff(date.time)*rollmean(value,2))


```

## 

```{r data setup mess it up}

data <- read.csv("Copy_C2A_R0_air_i106_2021.csv")

data$date.time <- mdy_hms(data$date.time, tz = "America/Chicago", truncated = 3) #Get R to recognize your current date format
  data$date.time <- as.POSIXlt((round(data$date.time, "mins")), tz = "America/Chicago") #Round off the seconds 

head(data$date.time) 
class(data$date.time)

# -- subset by date, add week-counting columns, calculate weekly means --

START <- as.POSIXlt("2020-10-01 00:00:00", tz = "") #trying the october 1-yr set to see if the code works. 5-5, SD
#BREAK <- as.POSIXlt("2020-08-25", tz = "")
#RESUME <- as.POSIXlt("2020-09-30", tz = "")
END <- as.POSIXlt("2021-10-01 00:00:00", tz = "")

data[is.na(data$value),] %>% #there should be a handful of NA's present if you're working with the right version of ALL.csv.
  print()

data <- data %>% 
  subset.data.frame(date.time >= START & date.time <= END)  #subset the whole df                         
  

# remove partial datasets for the given window. Anything with less than 500 values gets removed. 
# not needed for this subset I'm working on
data <- data %>%
  group_by(site, rep, position) %>% 
  mutate(length = length(date.time)) %>% 
  filter(length > 500)

view(data)
str(data)

```




```{r first try}

#Best first guess:
#5/11/2022: it worked with the test data!
AUC.mat <- diff(data$date.time)*rollmean(data$value,2)

AUC <- sum(diff(data$date.time)*rollmean(data$value,2), na.rm = TRUE) #make sure to set na.rm = TRUE to remove NA values.
view(AUC.mat)
AUC

```

##Now, move this to the entirety of ALL.csv

```{r}

#set entire row to NA if <= 0 or NA in value column
#check to see if it worked.
#   for the Oct 2020-Sept 2021 dataset, it's 40.5K out of 206.2k total rows.
head(data)
temps.df <- data %>%  ungroup()
str(temps.df)
temps.df[temps.df$value <= 0 | is.na(temps.df$value),] <- NA  
num.nanas <- temps.df[is.na(temps.df$date.time),]
print(length(c(num.nanas$date.time))) 

# WARNING: Potentially Time-intensive calculations below!!!

#summarise after grouping by site, rep, position.
#keep these columns: site, rep, position
#  create degree.days column by outputting the sum of all polygons for each position. (Not sensor, but position)
#only keep rows with distinct value for site, rep, position. Remove all others.
#5/12: currently having problems with this code. Output gives positive and negative values.

DH.df1 <- temps.df %>%   
  group_by(site, rep, position) %>% 
  summarise(site = site, 
            rep = rep,
            position = position, 
            degree.hours = sum(abs(diff(hour(date.time)))*rollmean(value,2), na.rm = TRUE)) %>% 
  distinct(site, rep, position, .keep_all = TRUE) %>% 
  ungroup()
DH.df1$degree.days <- DH.df1$degree.hours / 24

view(DH.df1)
str(DH.df1)
write_csv(DH.df1, file = "AUC_degree_hours_prelim.csv" )


#setting up the monthly grouping dataframe

DH.df2 <- temps.df %>% 
  mutate(year.mon = format(as.POSIXlt(date.time), "%Y-%m")) %>% 
  group_by(site, rep, position, year.mon) %>% 
  summarize(month.DH = sum(abs(diff(hour(date.time)))*rollmean(value,2), na.rm = TRUE)) %>% 
  ungroup()
DH.df2$month.DD <- signif(DH.df2$month.DH / 24, 3) %>%
  format(digits = 2, scientific = FALSE)
args(signif)
view(DH.df2)

DH.df2 <- DH.df2 %>% 
  pivot_wider(names_from = year.mon, values_from = c(month.DH, month.DD)) #puts year.mon into their own columns

degree.hours.dfW <- left_join(DH.df1, DH.df2)

view(degree.hours.dfW)

write_csv(degree.hours.dfW, "degree_hours_OCT_v1.csv") #write the csv file.




```

