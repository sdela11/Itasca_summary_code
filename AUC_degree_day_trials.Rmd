---
title: "AUC_degree_day_trials"
author: "Sara DeLaurentis"
date: "5/11/2022"
output: html_document
---

```{r setup, include=FALSE}

library(stringr)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(zoo)


```

## 

```{r data setup}

## TEMPORARILY SET WORKING DIRECTORY TO "Itasca_project_19-21" !!!! ##

data <- read.csv("ALL.csv")

head(data)

data$date.time <- data$date.time %>% as.POSIXlt(tz = "") #set date/time class to POSIXlt for greater ease in parsing date elements.
head(data$date.time) 

# -- subset by date, add week-counting columns, calculate weekly means --

START <- as.POSIXlt("2020-10-01 00:00:00", tz = "") #trying the october 1-yr set to see if the code works. 5-5, SD
#BREAK <- as.POSIXlt("2020-08-25", tz = "")
#RESUME <- as.POSIXlt("2020-09-30", tz = "")
END <- as.POSIXlt("2021-10-01 00:00:00", tz = "")

data[is.na(data$value),] %>% #there should be a handful of NA's present if you're working with the right version of .csv files.
  print()

data <- data %>% 
  subset.data.frame(date.time >= START & date.time <= END) #subset the whole df                         
    # Apply filter & !is.na

# remove partial datasets for the given window. Anything with less than 500 values gets removed.
data <- data %>%
  group_by(site, rep, position) %>% 
  mutate(length = length(date.time)) %>% 
  filter(length > 500)


```


```{r AUC trial one}

#  https://stackoverflow.com/questions/4954507/calculate-the-area-under-a-curve

x <- 1:10
y <- 3*x+25 #y = 3 times x, +25.
id <- order(x) #an ordered x
print(id)
str(id)
AUC <- sum(diff(x[id])*rollmean(y[id],2)) 

AUC.mat <- c(diff(x[id])*rollmean(y[id],2))
view(AUC.mat)

#translation: the sum of: the lagged (by 1) difference of x[id] times the mean of the two y values. Here, the difference is only 1. Each polygon is 1 wide by the y-mean high.

#how to adapt this to date.time measurements taken every 4 hours?
#each polygon is the time difference by the mean between the two temperature points high.

print(AUC)
?rollmean


#Best first guess:

AUC <- sum(diff(date.time)*rollmean(value,2))


```

## 

```{r data setup mess it up}

data <- read.csv("Copy_C2A_R0_air_i106_2021.csv")

data$date.time <- mdy_hms(data$date.time, tz = "America/Chicago", truncated = 3) #Get R to recognize your current date format
  data$date.time <- as.POSIXlt((round(data$date.time, "mins")), tz = "America/Chicago") #Round off the seconds 

head(data$date.time) 
class(data$date.time)

# -- subset by date, add week-counting columns, calculate weekly means --

START <- as.POSIXlt("2020-10-01 00:00:00", tz = "") #trying the october 1-yr set to see if the code works. 5-5, SD
#BREAK <- as.POSIXlt("2020-08-25", tz = "")
#RESUME <- as.POSIXlt("2020-09-30", tz = "")
END <- as.POSIXlt("2021-10-01 00:00:00", tz = "")

data[is.na(data$value),] %>% #there should be a handful of NA's present if you're working with the right version of ALL.csv.
  print()

data <- data %>% 
  subset.data.frame(date.time >= START & date.time <= END)  #subset the whole df                         
  

# remove partial datasets for the given window. Anything with less than 500 values gets removed. 
# not needed for this subset I'm working on
data <- data %>%
  group_by(site, rep, position) %>% 
  mutate(length = length(date.time)) %>% 
  filter(length > 500)

data %>%  ungroup()
data[data$value <= 0, ] <- NA
view(data)

str(data)

```




```{r first try}

#Best first guess:

AUC.mat <- diff(data$date.time)*rollmean(data$value,2)

AUC <- sum(diff(data$date.time)*rollmean(data$value,2), na.rm = TRUE) #make sure to set na.rm = TRUE to remove NA values.
view(AUC.mat)
AUC

```

#Now, move this to the entirety of ALL.csv
```{r}

degree.hours <- data %>%  ungroup()
str(degree.hours)
degree.hours[degree.hours$value <= 0 | is.na(degree.hours$value),] <- NA

#check to see if it worked:
no.nanas <- degree.hours[is.na(degree.hours$date.time),]
print(length(c(no.nanas$date.time)))


degree.hours <- degree.hours %>%   
  group_by(site, rep, position) %>% 
  summarise(site = site, #keep these columns: site, rep, position
            rep = rep,
            position = position, #create degree.days column by outputting the sum of meantemp for each position. (Not sensor, but position)
            degree.hours = sum(diff(data$date.time)*rollmean(data$value,2), na.rm = TRUE)) %>% 
  distinct(site, rep, position, .keep_all = TRUE) %>% 
  ungroup()
#view(degree.hours)

degree.hours$degree.hours %>% formatC(format = "e", digits = 4)


DD.df2 <- daily.means.df %>% 
  mutate(year.mon = format(as.Date(date), "%Y-%m")) %>% 
  group_by(site, rep, position, year.mon) %>% 
  summarize(month.GDD = sum(meantemp)) %>% 
  ungroup()
view(DD.df2)

DD.df2 <- DD.df2 %>% 
  pivot_wider(names_from = year.mon, values_from = month.GDD) #puts year.mon into their own columns

degree.days.dfW <- left_join(DD.df1, DD.df2)

view(degree.days.dfW)

write_csv(degree.days.dfW, "degree_days_OCT_v2.csv") #write the csv file.




```

